{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 3 - NIKO PATERNITI BARBINO 638257"
      ],
      "metadata": {
        "id": "OGosoj7FkVuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement from scratch an RBM and apply it to DSET3. The RBM should be implemented fully by you (both CD-1 training and inference steps) but you are free to use library functions for the rest (e.g. image loading and management, etc.).**\n",
        "\n",
        "1.     Train an RBM with a number of hidden neurons selected by you (single layer) on the MNIST data (use the training set split provided by the website).\n",
        "\n",
        "2.     Use the trained RBM to encode a selection of test images (e.g. using one per digit type) using the corresponding activation of the hidden neurons.\n",
        "\n",
        "3.     Reconstruct the original test images from their hidden encoding and confront the reconstructions with the original image (use a suitable quantitative metric to assess the reconstraction quality and also choose few examples to confront visually)."
      ],
      "metadata": {
        "id": "i69LzZ0wKTyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ],
      "metadata": {
        "id": "oZCa3qaJ96Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(x):  \n",
        "    return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "def flatten(x):\n",
        "    return x.reshape(x.shape[0], -1)"
      ],
      "metadata": {
        "id": "FPv6iwjl93Lv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RBM IMPLEMENTATION\n"
      ],
      "metadata": {
        "id": "KCJGIz4NkxGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented the RBM randomly initializing the weights and initializing the bias to zero for each hidden and visible unit.\n",
        "Follows the implementation of the CD-1 Trainining algorithm"
      ],
      "metadata": {
        "id": "1PNexiLC7x6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class RBM:  \n",
        "    def __init__(self, hidden_units, visible_units):  \n",
        "        self.nh = hidden_units  \n",
        "        self.nv = visible_units\n",
        "        self.weights = np.random.uniform(-1/self.nv, 1/self.nv, (self.nv, self.nh))  \n",
        "        self.bias_h = np.zeros(self.nh)  \n",
        "        self.bias_v = np.zeros(self.nv) \n",
        "        print(\"Built a RBM with \" + str(self.nv) + \" visible units and \" + str(self.nh) + \" hidden units\")\n",
        "\n",
        "    def train(self, Xtr, epochs = 100, learning_rate = 0.1):  # CD-1 training algorithm\n",
        "        n = 6000  # batch size\n",
        "\n",
        "        print(\"Training on \" + str(n) + \" random elements for \" + str(epochs) + \" epochs\")\n",
        "        for epoch in range(epochs):\n",
        "            idx = np.random.uniform(low = 0, high = Xtr.shape[0], size=n).astype(int)\n",
        "            cXtr = Xtr[idx,:]\n",
        "\n",
        "            # Hidden probability\n",
        "            h_prob = logistic(np.dot(cXtr, self.weights) + self.bias_h)\n",
        "            wake = np.dot(cXtr.T, h_prob)\n",
        "\n",
        "            # Hidden states\n",
        "            h_state = h_prob > np.random.rand(n, self.nh)\n",
        "\n",
        "            # Reconstruction probability\n",
        "            reconstruction_data_prob = logistic(np.dot(h_state, self.weights.T) + self.bias_v)\n",
        "\n",
        "            # Reconstructed data\n",
        "            reconstruction_data = reconstruction_data_prob > np.random.rand(n, self.nv)\n",
        "            h_neg_prob = logistic(np.dot(reconstruction_data, self.weights) + self.bias_h)\n",
        "            dream = np.dot(reconstruction_data.T, h_neg_prob)\n",
        "\n",
        "            # Learning phase\n",
        "            error = np.sum((cXtr - reconstruction_data)**2)/n\n",
        "            dW = (wake - dream)/n\n",
        "            dBh = (np.sum(h_prob) - np.sum(h_neg_prob))/n\n",
        "            dBv = (np.sum(cXtr) - np.sum(reconstruction_data))/n\n",
        "            self.weights += learning_rate*dW\n",
        "            self.bias_h += learning_rate*dBh\n",
        "            self.bias_v += learning_rate*dBv\n",
        "            print(\"\\rError:\\t\" + \"{:.5f}\".format(error), end=\"\")\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "    def get_hidden_activations(self, Xtr): \n",
        "        n = Xtr.shape[0]\n",
        "\n",
        "        print(\"Computing hidden activations for \" + str(n) + \" elements\")\n",
        "        h_states = np.ones((n, self.nh))\n",
        "\n",
        "        h_prob = logistic(np.dot(Xtr, self.weights) + self.bias_h)\n",
        "        h_states[:,:] = h_prob > np.random.rand(n, self.nh)\n",
        "        return h_states"
      ],
      "metadata": {
        "id": "dAH16K16KYMr"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the dataset from keras and compute the hidden activations"
      ],
      "metadata": {
        "id": "V6xMJ3Nx9VYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(str(X_train.shape[0]) + \" images of \" + str(X_train.shape[1]) + \"x\" + str(X_train.shape[2]) + \" pixels =>\", end = \" \")\n",
        "Xtr = flatten(X_train)\n",
        "Xts = flatten(X_test)\n",
        "print(str(Xtr.shape[0]) + \" vectors \" + str(Xtr.shape[1]) + \" elements\")\n",
        "\n",
        "rbm = RBM(visible_units = (28*28), hidden_units = 500)\n",
        "rbm.train(Xtr, epochs=100, learning_rate=0.01) \n",
        "X_train_h = rbm.get_hidden_activations(Xtr)\n",
        "X_test_h = rbm.get_hidden_activations(Xts)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7crHJLmxN1HO",
        "outputId": "083daadd-1bf7-4b27-c6d3-3cd5f7c4d5ab"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 images of 28x28 pixels => 60000 vectors 784 elements\n",
            "Built a RBM with 784 visible units and 500 hidden units\n",
            "Training on 6000 random elements for 100 epochs\n",
            "Error:\t11348.19600\n",
            "Computing hidden activations for 60000 elements\n",
            "Computing hidden activations for 10000 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruct the original test images from their hidden encoding and confront the reconstructions with the original image (use a suitable quantitative metric to assess the reconstraction quality and also choose few examples to confront visually)."
      ],
      "metadata": {
        "id": "KfQxeyB_jnys"
      }
    }
  ]
}